{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIm0OmusVgXS"
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "652KpBBeANzx"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ng6rf-7xygn"
   },
   "source": [
    "# Data Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42vYsEBkx3ED"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Transformer:\n",
    "\n",
    "    def __init__(self, decision_size, decision_overlap, segments_size=90, segments_overlap=45, sampling=2):\n",
    "        self.segments_size = segments_size\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.sampling = sampling\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "\n",
    "    def transfer(self, dataset, features, method):\n",
    "        print(\"segmenting data with \" + str(len(dataset)) + \" points\")\n",
    "        segments, labels = self.__segment_signal(dataset, features)\n",
    "        print(\"making \" + str(len(segments)) + \" segments\")\n",
    "        if method == \"table\":\n",
    "            segments_dataset = self.__transfer_table(segments, features)\n",
    "        elif method == \"1d\":\n",
    "            segments_dataset = self.__transfer_1d(segments, features)\n",
    "        elif method == \"2d\":\n",
    "            segments_dataset = self.__transfer_2d(segments, features)\n",
    "        elif method == \"3d_1ch\":\n",
    "            segments_dataset = self.__transfer_2d_1ch(segments, features)\n",
    "        elif method == \"3d\":\n",
    "            segments_dataset = self.__transfer_3d(segments, features)\n",
    "        elif method == \"4d\":\n",
    "            segments_dataset = self.__transfer_4d(segments, features)\n",
    "        elif method == \"rnn_2d\":\n",
    "            segments_dataset, labels = self.__transfer_rnn_2d(segments, labels, features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            segments_dataset, labels = self.__transfer_rnn_3d_1ch(segments, labels, features)\n",
    "        return segments_dataset, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def data_shape(method, n_features, segments_size, segments_overlap=None, decision_size=None):\n",
    "        if method == \"table\":\n",
    "            return (None, n_features * segments_size)\n",
    "        elif method == \"1d\":\n",
    "            return (None, 1, n_features * segments_size, 1)\n",
    "        elif method == \"2d\":\n",
    "            return (None, n_features, segments_size)\n",
    "        elif method == \"3d_1ch\":\n",
    "            return (None, n_features, segments_size, 1)\n",
    "        elif method == \"3d\":\n",
    "            return (None, 1, segments_size, n_features)\n",
    "        elif method == \"4d\":\n",
    "            return (n_features, None, 1, segments_size, 1)\n",
    "        elif method == \"rnn_2d\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, segments_size * n_features)\n",
    "        elif method == \"rnn_3d_1ch\":\n",
    "            s_b = Transformer.get_segments_a_decision_window(segments_size,\n",
    "                                                             int(segments_size * segments_overlap),\n",
    "                                                             decision_size)\n",
    "            return (None, s_b, 1, segments_size * n_features)\n",
    "        return ()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_segments_a_decision_window(segment_size, segment_overlap_size, decision_size):\n",
    "        return int((decision_size - segment_size) / (segment_size - segment_overlap_size) + 1)\n",
    "\n",
    "    def __transfer_rnn_2d(self, segments, labels, features):\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "            subset = segments[np.where(labels == _id)]\n",
    "            n = subset.shape[0]\n",
    "            o = int(np.floor((n - r) / (s - r)))\n",
    "            for i in range(o):\n",
    "                row = []\n",
    "                for j in range(s):\n",
    "                    A = subset[i * (s - r) + j]\n",
    "                    A = A.reshape(A.shape[0] * A.shape[1])\n",
    "                    row.append(A)\n",
    "                y_output.append(_id)\n",
    "                x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_rnn_3d_1ch(self, segments, labels, features):\n",
    "        # (samples, time, channels=1, rows)\n",
    "        y_output = []\n",
    "        x_output = []\n",
    "        c = len(np.unique(labels))\n",
    "        s = Transformer.get_segments_a_decision_window(self.segments_size,\n",
    "                                                       self.segments_overlap,\n",
    "                                                       self.decision_size)\n",
    "        r = int(np.floor(s * self.decision_overlap))\n",
    "        for _id in np.unique(labels):\n",
    "            subset = segments[np.where(labels == _id)]\n",
    "            n = subset.shape[0]\n",
    "            o = int(np.floor((n - r) / (s - r)))\n",
    "            for i in range(o):\n",
    "                row = []\n",
    "                for j in range(s):\n",
    "                    A = subset[i * (s - r) + j]\n",
    "                    A = A.reshape(A.shape[0] * A.shape[1])\n",
    "                    row.append([A])\n",
    "                y_output.append(_id)\n",
    "                x_output.append(row)\n",
    "        x_output = np.array(x_output)\n",
    "        y_output = np.array(y_output)\n",
    "        return x_output, y_output\n",
    "\n",
    "    def __transfer_table(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_1d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                for i in range(len(segment[feature_i])):\n",
    "                    row.append(segment[feature_i][i])\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_2d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_3d_1ch(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for feature_i in range(len(features)):\n",
    "                row.append(segment[feature_i])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return np.expand_dims(new_dataset, axis=3)\n",
    "\n",
    "    def __transfer_3d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for segment in segments:\n",
    "            row = []\n",
    "            for i in range(len(segment[0])):\n",
    "                cell = []\n",
    "                for feature_i in range(len(features)):\n",
    "                    cell.append(segment[feature_i][i])\n",
    "                row.append(cell)\n",
    "            new_dataset.append([row])\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __transfer_4d(self, segments, features):\n",
    "        new_dataset = []\n",
    "        for feature_i in range(len(features)):\n",
    "            row = []\n",
    "            for segment in segments:\n",
    "                cell = []\n",
    "                for element in segment[feature_i]:\n",
    "                    cell.append([element])\n",
    "                row.append([cell])\n",
    "            new_dataset.append(row)\n",
    "\n",
    "        new_dataset = np.array(new_dataset)\n",
    "        return new_dataset\n",
    "\n",
    "    def __windows(self, data):\n",
    "        start = 0\n",
    "        while start < data.count():\n",
    "            yield int(start), int(start + self.segments_size)\n",
    "            start += (self.segments_size - self.segments_overlap)\n",
    "\n",
    "    def __segment_signal(self, dataset, features):\n",
    "        segments = []\n",
    "        labels = []\n",
    "        for class_i in np.unique(dataset[\"id\"]):\n",
    "            subset = dataset[dataset[\"id\"] == class_i]\n",
    "            for (start, end) in self.__windows(subset[\"id\"]):\n",
    "                feature_slices = []\n",
    "                for feature in features:\n",
    "                    feature_slices.append(subset[feature][start:end].tolist())\n",
    "                if len(feature_slices[0]) == self.segments_size:\n",
    "                    segments.append(feature_slices)\n",
    "                    labels.append(class_i)\n",
    "        return np.array(segments), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbUrAsY1enTC"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdYZjcS33pzd"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self, db_path, sample_rate, features,\n",
    "                 window_time, window_overlap_percentage,\n",
    "                 decision_time, decision_overlap_percentage,\n",
    "                 add_noise, noise_rate,\n",
    "                 train_blocks: list, valid_blocks: list, test_blocks: list,\n",
    "                 data_length_time=-1):\n",
    "        \"\"\"\n",
    "        :param db_path:\n",
    "        :param sample_rate:\n",
    "        :param features:\n",
    "        :param window_time: in seconds\n",
    "        :param window_overlap_percentage: example: 0.75 for 75%\n",
    "        :param add_noise: True or False\n",
    "        :param noise_rate:\n",
    "        :param train_blocks:\n",
    "        :param valid_blocks:\n",
    "        :param test_blocks:\n",
    "        :param data_length_time: the amount of data from each class in seconds. -1 means whole existing data.\n",
    "        \"\"\"\n",
    "        self.db_path = db_path\n",
    "        self.features = features\n",
    "        self.sample_rate = sample_rate\n",
    "        self.window_size = window_time * sample_rate\n",
    "        self.window_overlap_size = int(self.window_size * window_overlap_percentage)\n",
    "        self.decision_size = decision_time * sample_rate\n",
    "        self.decision_overlap_size = int(self.decision_size * decision_overlap_percentage)\n",
    "        self.decision_overlap_percentage = decision_overlap_percentage\n",
    "        self.add_noise = add_noise\n",
    "        self.noise_rate = noise_rate\n",
    "        self.train_blocks = train_blocks\n",
    "        self.valid_blocks = valid_blocks\n",
    "        self.test_blocks = test_blocks\n",
    "        self.data_length_size = data_length_time * sample_rate if data_length_time != -1 else -1\n",
    "\n",
    "        # Initialization\n",
    "        self.train_dataset = pd.DataFrame()\n",
    "        self.valid_dataset = pd.DataFrame()\n",
    "        self.test_dataset = pd.DataFrame()\n",
    "        self.n_train_dataset = pd.DataFrame()\n",
    "        self.n_valid_dataset = pd.DataFrame()\n",
    "        self.n_test_dataset = pd.DataFrame()\n",
    "        self.X_train = np.array([])\n",
    "        self.y_train = np.array([])\n",
    "        self.X_valid = np.array([])\n",
    "        self.y_valid = np.array([])\n",
    "        self.X_test = np.array([])\n",
    "        self.y_test = np.array([])\n",
    "\n",
    "    def load_data(self, n_classes, method):\n",
    "        segments_path = self.db_path + \\\n",
    "                        \"segments/\" + \\\n",
    "                        \"method: \" + str(method) + os.sep + \\\n",
    "                        \"wl: \" + str(self.window_size) + os.sep + \\\n",
    "                        \"wo: \" + str(self.window_overlap_size) + os.sep + \\\n",
    "                        \"dl: \" + str(self.decision_size) + os.sep + \\\n",
    "                        \"do: \" + str(self.decision_overlap_size) + os.sep + \\\n",
    "                        \"train: \" + str(self.train_blocks) + os.sep + \\\n",
    "                        \"valid: \" + str(self.valid_blocks) + os.sep + \\\n",
    "                        \"test: \" + str(self.test_blocks) + os.sep\n",
    "        if os.path.exists(segments_path + 'X_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_train.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_valid.npy') \\\n",
    "                and os.path.exists(segments_path + 'X_test.npy') \\\n",
    "                and os.path.exists(segments_path + 'y_test.npy'):\n",
    "            print(\"Dataset is already\")\n",
    "            self.X_train = np.load(segments_path + 'X_train.npy')\n",
    "            self.y_train = np.load(segments_path + 'y_train.npy')\n",
    "            self.X_valid = np.load(segments_path + 'X_valid.npy')\n",
    "            self.y_valid = np.load(segments_path + 'y_valid.npy')\n",
    "            self.X_test = np.load(segments_path + 'X_test.npy')\n",
    "            self.y_test = np.load(segments_path + 'y_test.npy')\n",
    "        else:\n",
    "            self.__preprocess(n_classes, method)\n",
    "            # Save Dataset\n",
    "            if not os.path.exists(segments_path):\n",
    "                os.makedirs(segments_path)\n",
    "            np.save(segments_path + 'X_train.npy', self.X_train)\n",
    "            np.save(segments_path + 'y_train.npy', self.y_train)\n",
    "            np.save(segments_path + 'X_valid.npy', self.X_valid)\n",
    "            np.save(segments_path + 'y_valid.npy', self.y_valid)\n",
    "            np.save(segments_path + 'X_test.npy', self.X_test)\n",
    "            np.save(segments_path + 'y_test.npy', self.y_test)\n",
    "\n",
    "        def to_dic(data):\n",
    "            dic = {}\n",
    "            for i, x in enumerate(data):\n",
    "                dic[str(i)] = x\n",
    "            return dic\n",
    "\n",
    "        if len(self.X_train.shape) == 5:\n",
    "            self.X_train = to_dic(self.X_train)\n",
    "            self.X_valid = to_dic(self.X_valid)\n",
    "            self.X_test = to_dic(self.X_test)\n",
    "\n",
    "    def __preprocess(self, n_classes, method):\n",
    "        csv_paths = np.random.choice(glob.glob(self.db_path + \"*.csv\"), n_classes, replace=False)\n",
    "\n",
    "        self.class_names = {}\n",
    "        for i, csv_path in enumerate(csv_paths):\n",
    "            label = os.path.basename(csv_path).split('.')[0]\n",
    "            self.class_names[label] = i\n",
    "            train, valid, test = self.__read_data(csv_path, self.features, label)\n",
    "            train['id'] = i\n",
    "            valid['id'] = i\n",
    "            test['id'] = i\n",
    "            self.train_dataset = pd.concat([self.train_dataset, train])\n",
    "            self.valid_dataset = pd.concat([self.valid_dataset, valid])\n",
    "            self.test_dataset = pd.concat([self.test_dataset, test])\n",
    "\n",
    "        self.__standardization()\n",
    "        self.__segmentation(method=method)\n",
    "\n",
    "    def __read_data(self, path, features, label):\n",
    "        data = pd.read_csv(path, low_memory=False)\n",
    "        data = data[features]\n",
    "        data = data.fillna(data.mean())\n",
    "        length = self.data_length_size if self.data_length_size != -1 else data.shape[0]\n",
    "        print('class: %5s, data size: %s, selected data size: %s' % (\n",
    "            label, str(timedelta(seconds=int(data.shape[0] / self.sample_rate))),\n",
    "            str(timedelta(seconds=int(length / self.sample_rate)))))\n",
    "        return self.__split_to_train_valid_test(data)\n",
    "\n",
    "    def __split_to_train_valid_test(self, data):\n",
    "        n_blocks = max(self.train_blocks + self.valid_blocks + self.test_blocks) + 1\n",
    "        block_length = int(len(data[:self.data_length_size]) / n_blocks)\n",
    "\n",
    "        train_data = pd.DataFrame()\n",
    "        for i in range(len(self.train_blocks)):\n",
    "            start = self.train_blocks[i] * block_length\n",
    "            end = self.train_blocks[i] * block_length + block_length - 1\n",
    "            if train_data.empty:\n",
    "                train_data = data[start:end]\n",
    "            else:\n",
    "                train_data = pd.concat([data[start:end], train_data])\n",
    "\n",
    "        valid_data = pd.DataFrame()\n",
    "        for i in range(len(self.valid_blocks)):\n",
    "            start = self.valid_blocks[i] * block_length\n",
    "            end = self.valid_blocks[i] * block_length + block_length - 1\n",
    "            if valid_data.empty:\n",
    "                valid_data = data[start:end]\n",
    "            else:\n",
    "                valid_data = pd.concat([data[start:end], valid_data])\n",
    "\n",
    "        test_data = pd.DataFrame()\n",
    "        for i in range(len(self.test_blocks)):\n",
    "            start = self.test_blocks[i] * block_length\n",
    "            end = self.test_blocks[i] * block_length + block_length - 1\n",
    "            if test_data.empty:\n",
    "                test_data = data[start:end]\n",
    "            else:\n",
    "                test_data = pd.concat([data[start:end], test_data])\n",
    "\n",
    "        if self.add_noise:\n",
    "            test_data = self.__add_noise_to_data(test_data)\n",
    "\n",
    "        return train_data, valid_data, test_data\n",
    "\n",
    "    def __add_noise_to_data(self, x):\n",
    "        x_power = x ** 2\n",
    "        sig_avg_watts = np.mean(x_power)\n",
    "        sig_avg_db = 10 * np.log10(sig_avg_watts)\n",
    "        noise_avg_db = sig_avg_db - self.target_snr_db\n",
    "        noise_avg_watts = 10 ** (noise_avg_db / 10)\n",
    "        mean_noise = 0\n",
    "        noise_volts = np.random.normal(mean_noise, np.sqrt(noise_avg_watts), size=x.shape)\n",
    "        return x + noise_volts\n",
    "\n",
    "    def __standardization(self):\n",
    "        scaler = preprocessing.StandardScaler()\n",
    "        scaler = scaler.fit(self.train_dataset.iloc[:, :-1])\n",
    "        n_train_dataset = scaler.transform(self.train_dataset.iloc[:, :-1])\n",
    "        n_valid_dataset = scaler.transform(self.valid_dataset.iloc[:, :-1])\n",
    "        n_test_dataset = scaler.transform(self.test_dataset.iloc[:, :-1])\n",
    "\n",
    "        self.n_train_dataset = pd.DataFrame(n_train_dataset, columns=self.features)\n",
    "        self.n_valid_dataset = pd.DataFrame(n_valid_dataset, columns=self.features)\n",
    "        self.n_test_dataset = pd.DataFrame(n_test_dataset, columns=self.features)\n",
    "        self.n_train_dataset['id'] = self.train_dataset.iloc[:, -1].tolist()\n",
    "        self.n_valid_dataset['id'] = self.valid_dataset.iloc[:, -1].tolist()\n",
    "        self.n_test_dataset['id'] = self.test_dataset.iloc[:, -1].tolist()\n",
    "\n",
    "    def __segmentation(self, method):\n",
    "        transformer = Transformer(segments_size=self.window_size,\n",
    "                                  segments_overlap=self.window_overlap_size,\n",
    "                                  decision_size=self.decision_size,\n",
    "                                  decision_overlap=self.decision_overlap_percentage)\n",
    "        self.X_train, self.y_train = transformer.transfer(self.n_train_dataset, self.features, method=method)\n",
    "        self.X_valid, self.y_valid = transformer.transfer(self.n_valid_dataset, self.features, method=method)\n",
    "        self.X_test, self.y_test = transformer.transfer(self.n_test_dataset, self.features, method=method)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kn2SeTarrYuU"
   },
   "source": [
    "# Scoring Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pbs(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = (c - 1) / (c ** 2)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    return tf.math.reduce_mean(tf.math.reduce_mean(tf.math.square(tf.math.subtract(y, q)), axis=1) + payoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs(y,q):\n",
    "    return tf.math.reduce_mean(tf.keras.losses.mean_squared_error(y,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pll(y, q):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    c = y.get_shape()[1]\n",
    "\n",
    "    ST = tf.math.subtract(q, tf.reduce_sum(tf.where(y == 1, q, y), axis=1)[:, None])\n",
    "    ST = tf.where(ST < 0, tf.constant(0, dtype=tf.float32), ST)\n",
    "    payoff = tf.reduce_sum(tf.math.ceil(ST), axis=1)\n",
    "    M = math.log(1/c)\n",
    "    payoff = tf.where(payoff > 0, tf.constant(M, dtype=tf.float32), payoff)\n",
    "    log_loss = tf.keras.losses.categorical_crossentropy(y,q)\n",
    "    p_log_loss = tf.cast(log_loss, tf.float32) - payoff\n",
    "    return tf.math.reduce_mean(p_log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ll(y,q):\n",
    "    return tf.math.reduce_mean(tf.keras.losses.categorical_crossentropy(y,q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ddmcuoxK-Ij"
   },
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KXchGve9LAIy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_metric(y_true, y_pred):\n",
    "    # Define the true positives, false positives and false negatives\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    # Calculate the precision and recall\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    # Calculate the F1 score\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_w5A0xO9Upk1"
   },
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMV3XF69j-yx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "class Classifier():\n",
    "    def __init__(self, loss_function):\n",
    "        self.loss_function = loss_function\n",
    "\n",
    "    def get_loss_function(self):\n",
    "        return self.loss_function\n",
    "\n",
    "    def get_loss_function_name(self):\n",
    "        return self.loss_function if type(self.loss_function) == str else self.loss_function.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEJcOe1UjICL"
   },
   "outputs": [],
   "source": [
    "# Convolutinal Neural Network\n",
    "class CNN_L(Classifier):\n",
    "    def __init__(self, classes, n_features,\n",
    "                 segments_size, segments_overlap,\n",
    "                 decision_size, decision_overlap,\n",
    "                 loss_function, loss_metric):\n",
    "        super().__init__(loss_metric)\n",
    "        self.classes = classes\n",
    "        self.n_features = n_features\n",
    "        self.segments_size = segments_size\n",
    "        self.input_shape = self.get_input_shape()\n",
    "        self.segments_overlap = segments_overlap\n",
    "        self.decision_size = decision_size\n",
    "        self.decision_overlap = decision_overlap\n",
    "        self.initializer = tf.keras.initializers.GlorotNormal(seed=42)\n",
    "\n",
    "        # Build and compile the model\n",
    "        self.model = self.build_model_l()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Nadam(0.001, 0.9)\n",
    "        self.model.compile(loss=loss_function,\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy',\n",
    "                                    tf.keras.metrics.Precision(),\n",
    "                                    tf.keras.metrics.Recall(),\n",
    "                                    f1_metric,\n",
    "                                    loss_metric])\n",
    "        \n",
    "#         self.model.summary()\n",
    "\n",
    "    def get_input_shape(self):\n",
    "        data_shape = Transformer.data_shape(method=self.get_data_arrangement(), n_features=self.n_features,\n",
    "                                            segments_size=self.segments_size)\n",
    "        return data_shape[-3], data_shape[-2], data_shape[-1]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data_arrangement():\n",
    "        return \"3d\"\n",
    "\n",
    "    def count_params(self):\n",
    "        return self.model.count_params()\n",
    "\n",
    "    def build_model_l(self):\n",
    "        input_ = tf.keras.layers.Input(shape=self.input_shape)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer)(input_)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(32, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        x = tf.keras.layers.Conv2D(16, kernel_size=(1, 3), strides=1, padding=\"same\", activation='relu', kernel_initializer=self.initializer)(x)\n",
    "        x = tf.keras.layers.BatchNormalization(momentum=0.9)(x)\n",
    "        x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "\n",
    "        dense = tf.keras.layers.Flatten()(x)\n",
    "        dense = tf.keras.layers.Dense(1024, kernel_initializer=self.initializer)(dense)\n",
    "        dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "        dense = tf.keras.layers.Dropout(0.2)(dense)\n",
    "        dense = tf.keras.layers.Dense(self.classes, activation='softmax')(dense)\n",
    "\n",
    "        model = tf.keras.models.Model(inputs=input_, outputs=[dense])\n",
    "        return model\n",
    "\n",
    "    def train(self, epochs, X_train, y_train, X_valid, y_valid, X_test, callback, batch_size=128):\n",
    "        # Change the labels from categorical to one-hot encoding\n",
    "        y_train_onehot = np.asarray(pd.get_dummies(y_train), dtype=np.int8)\n",
    "        y_valid_onehot = np.asarray(pd.get_dummies(y_valid), dtype=np.int8)\n",
    "\n",
    "        history = self.model.fit(X_train, y_train_onehot,\n",
    "                                 validation_data=(X_valid, y_valid_onehot),\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                 verbose=0,\n",
    "                                 shuffle=True,\n",
    "                                 callbacks=callback)\n",
    "\n",
    "        return history, self.model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bB8dITyHh5H"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwaxs5pGE05T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "\n",
    "def analysis_model(loss_fn, loss_name, y_pred, y_real_raw):\n",
    "    result = {}\n",
    "    result[loss_name] = loss_fn(np.asarray(pd.get_dummies(y_real_raw), dtype=np.int8),\n",
    "                                         np.asarray(y_pred, dtype=np.float64)).numpy()\n",
    "    y_pred_arg = np.argmax(y_pred, axis=1)\n",
    "    result['accuracy'] = accuracy_score(y_real_raw, y_pred_arg)\n",
    "    result['precision'] = precision_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['recall'] = recall_score(y_real_raw, y_pred_arg, average='macro')\n",
    "    result['f1'] = f1_score(y_real_raw, y_pred_arg, average='macro')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "caa9SgQ8HmsI"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8_3iDZgqvCJ1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def train_model(dataset: Dataset, classifier: Classifier, epochs, batch_size,\n",
    "                callback, monitor_metric, monitor_mode, log_dir):\n",
    "    if callback == \"ModelCheckpoint\":\n",
    "      # Define the checkpoint callback\n",
    "      reg_callback = ModelCheckpoint(filepath=log_dir+'/model_checkpoint.h5',\n",
    "                                            monitor=monitor_metric,\n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=True,\n",
    "                                            mode=monitor_mode,\n",
    "                                            verbose=0)\n",
    "    elif callback == \"EarlyStopping\":\n",
    "      # Define the early stopping callback\n",
    "      reg_callback = EarlyStopping(monitor=monitor_metric,\n",
    "                                              patience=10,\n",
    "                                              mode=monitor_mode,\n",
    "                                              verbose=0)\n",
    "\n",
    "    tbCallBack = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    history, y_test_prediction = classifier.train(epochs=epochs,\n",
    "                                                  X_train=dataset.X_train,\n",
    "                                                  y_train=dataset.y_train,\n",
    "                                                  X_valid=dataset.X_valid,\n",
    "                                                  y_valid=dataset.y_valid,\n",
    "                                                  X_test=dataset.X_test,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  callback=[tbCallBack, reg_callback])\n",
    "\n",
    "    result_test = analysis_model(loss_fn=classifier.get_loss_function(),\n",
    "                                 loss_name=classifier.get_loss_function_name(),\n",
    "                                 y_pred=y_test_prediction,\n",
    "                                 y_real_raw=dataset.y_test)\n",
    "\n",
    "    result_test['validation_metric'] = history.history['val_'+classifier.get_loss_function_name()][-1]\n",
    "\n",
    "    from scipy.stats import pearsonr\n",
    "    # Reverse the loss function by taking the negative of the loss values\n",
    "    r_func_loss = [-l for l in history.history['val_loss']]\n",
    "    r_metric_loss = [-l for l in history.history['val_'+classifier.get_loss_function_name()]]\n",
    "\n",
    "    # Calculate the Pearson's correlation coefficient between the reversed loss function and accuracy\n",
    "    corr_acc, p_value = pearsonr(r_func_loss, history.history['val_accuracy'])\n",
    "    corr_f1, p_value = pearsonr(r_func_loss, history.history['val_f1_metric'])\n",
    "    result_test['pearsonr_acc_func_loss'] = corr_acc\n",
    "    result_test['pearsonr_f1_func_loss'] = corr_f1\n",
    "    print(\"Pearson's correlation coefficient between BS function and accuracy:\", corr_acc, \" and f1:\", corr_f1)\n",
    "    corr_acc, p_value = pearsonr(r_metric_loss, history.history['val_accuracy'])\n",
    "    corr_f1, p_value = pearsonr(r_metric_loss, history.history['val_f1_metric'])\n",
    "    result_test['pearsonr_acc_metric_loss'] = corr_acc\n",
    "    result_test['pearsonr_f1_metric_loss'] = corr_f1\n",
    "    print(\"Pearson's correlation coefficient between PBS function and accuracy:\", corr_acc, \" and f1:\", corr_f1)\n",
    "\n",
    "    accuracy_func_loss, f1_func_loss, accuracy_metric_loss, f1_metric_loss = tops_intersection(1, r_func_loss, r_metric_loss, history.history['val_accuracy'], history.history['val_f1_metric'])\n",
    "    result_test['matching_top_1_accuracy_func_loss'] = accuracy_func_loss\n",
    "    result_test['matching_top_1_f1_func_loss'] = f1_func_loss\n",
    "    result_test['matching_top_1_accuracy_metric_loss'] = accuracy_metric_loss\n",
    "    result_test['matching_top_1_f1_metric_loss'] = f1_metric_loss\n",
    "\n",
    "    accuracy_func_loss, f1_func_loss, accuracy_metric_loss, f1_metric_loss = tops_intersection(3, r_func_loss, r_metric_loss, history.history['val_accuracy'], history.history['val_f1_metric'])\n",
    "    result_test['matching_top_3_accuracy_func_loss'] = accuracy_func_loss\n",
    "    result_test['matching_top_3_f1_func_loss'] = f1_func_loss\n",
    "    result_test['matching_top_3_accuracy_metric_loss'] = accuracy_metric_loss\n",
    "    result_test['matching_top_3_f1_metric_loss'] = f1_metric_loss\n",
    "\n",
    "    return result_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y2tRhVJb_KQ-"
   },
   "outputs": [],
   "source": [
    "def tops_intersection(top_n,r_func_loss, r_metric_loss, val_accuracy, val_f1_metric):\n",
    "    r_func_loss_tops = set(np.argsort(r_func_loss)[::-1][:top_n])\n",
    "    r_metric_loss_tops = set(np.argsort(r_metric_loss)[::-1][:top_n])\n",
    "    val_accuracy_tops = set(np.argsort(val_accuracy)[::-1][:top_n])\n",
    "    val_f1_metric_tops = set(np.argsort(val_f1_metric)[::-1][:top_n])\n",
    "\n",
    "    accuracy_func_loss = len(r_func_loss_tops.intersection(val_accuracy_tops))\n",
    "    f1_func_loss = len(r_func_loss_tops.intersection(val_f1_metric_tops))\n",
    "    accuracy_metric_loss = len(r_metric_loss_tops.intersection(val_accuracy_tops))\n",
    "    f1_metric_loss = len(r_metric_loss_tops.intersection(val_f1_metric_tops))\n",
    "    return accuracy_func_loss, f1_func_loss, accuracy_metric_loss, f1_metric_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS6yHM-MHosi"
   },
   "source": [
    "# Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SGLGGQfvHer"
   },
   "outputs": [],
   "source": [
    "def h_block_analyzer(db_path, sample_rate, features, n_classes, noise_rate,\n",
    "                     segments_time, segments_overlap,\n",
    "                     decision_time, decision_overlap,\n",
    "                     classifier, epochs, batch_size, data_length_time, repetitions,\n",
    "                     callback, monitor_metric, monitor_mode,\n",
    "                     n_h_block, n_train_h_block, n_valid_h_block, n_test_h_block, h_moving_step=1):\n",
    "    \"\"\"\n",
    "    :param db_path: the address of dataset directory\n",
    "    :param sample_rate: the sampling rate of signals\n",
    "    :param features: the signals of original data\n",
    "    :param n_classes: the number of classes\n",
    "    :param noise_rate: the rate of noises injected to test data\n",
    "    :param segments_time: the length of each segment in seconds.\n",
    "    :param segments_overlap: the overlap of each segment\n",
    "    :param classifier: the neural network\n",
    "    :param epochs: the number of training epochs\n",
    "    :param batch_size: the number of segments in each batch\n",
    "    :param data_length_time: the length of data of each class. -1 = whole.\n",
    "    :param repetitions: the number of repetitions for each h-block\n",
    "    :param n_h_block: the number of all hv blocks\n",
    "    :param n_train_h_block: the number of hv blocks to train network\n",
    "    :param n_valid_h_block: the number of hv blocks to validate network\n",
    "    :param n_test_h_block: the number of hv blocks to test network\n",
    "    :param h_moving_step: the number of movement of test and validation blocks in each iteration\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    final_statistics = {}\n",
    "    add_noise = noise_rate < 100\n",
    "\n",
    "    # Create hv blocks\n",
    "    data_blocks = [i for i in range(n_h_block)]\n",
    "    n_vt = (n_valid_h_block + n_test_h_block)\n",
    "    n_iteration = int((n_h_block - n_vt) / h_moving_step)\n",
    "\n",
    "    date_str = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    for i in range(n_iteration + 1):\n",
    "        statistics = {}\n",
    "        for j in range(repetitions):\n",
    "            print('iteration: %d/%d' % (i + 1, n_iteration + 1))\n",
    "            print('repetition: %d/%d' % (j + 1, repetitions))\n",
    "            training_container = data_blocks[0:i] + data_blocks[i + n_vt:n_h_block]\n",
    "            train_blocks = training_container[:n_train_h_block]\n",
    "            valid_blocks = data_blocks[i: i + n_valid_h_block]\n",
    "            test_blocks = data_blocks[i + n_valid_h_block: i + n_vt]\n",
    "            dataset = Dataset(db_path,\n",
    "                              sample_rate,\n",
    "                              features=features,\n",
    "                              window_time=segments_time,\n",
    "                              window_overlap_percentage=segments_overlap,\n",
    "                              decision_time=decision_time,\n",
    "                              decision_overlap_percentage=decision_overlap,\n",
    "                              add_noise=add_noise,\n",
    "                              noise_rate=noise_rate,\n",
    "                              train_blocks=train_blocks,\n",
    "                              valid_blocks=valid_blocks,\n",
    "                              test_blocks=test_blocks,\n",
    "                              data_length_time=data_length_time)\n",
    "            dataset.load_data(n_classes=n_classes, method=classifier.get_data_arrangement())\n",
    "            logdir = os.path.join(\"logs/checkpointss/\", date_str + \"[\" + str(i) + \"]/[\" + str(j) + \"]\")\n",
    "            if not os.path.exists(logdir):\n",
    "                os.makedirs(logdir)\n",
    "            result = train_model(dataset=dataset, classifier=classifier, epochs=epochs,\n",
    "                                 batch_size=batch_size,\n",
    "                                 callback=callback,\n",
    "                                 monitor_metric=monitor_metric,\n",
    "                                 monitor_mode=monitor_mode,\n",
    "                                 log_dir=logdir)\n",
    "            for key in result.keys():\n",
    "                if not key in statistics:\n",
    "                    statistics[key] = []\n",
    "                statistics[key].append(result[key])\n",
    "            shutil.rmtree(logdir, ignore_errors=True)\n",
    "\n",
    "        if monitor_mode == \"max\":\n",
    "            selected_index = np.nanargmax(statistics['validation_metric'])\n",
    "        else:\n",
    "            selected_index = np.nanargmin(statistics['validation_metric'])\n",
    "\n",
    "        for key in statistics.keys():\n",
    "            if not key in final_statistics:\n",
    "                final_statistics[key] = []\n",
    "            final_statistics[key].append(statistics[key][selected_index])\n",
    "        print(final_statistics)\n",
    "    return final_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqRhRm3V3SjM"
   },
   "source": [
    "# Save Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afgTXC083W14"
   },
   "outputs": [],
   "source": [
    "def save_result(log_dir, data: dict):\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Save to file\n",
    "    with open(log_dir + 'statistics.txt', 'a') as f:\n",
    "        f.write('\\n==========***==========\\n')\n",
    "        f.write(str(data))\n",
    "        f.write('\\n')\n",
    "\n",
    "    csv_file = log_dir + 'statistics.csv'\n",
    "    file_exists = os.path.isfile(csv_file)\n",
    "    try:\n",
    "        with open(csv_file, 'a') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            if not file_exists:\n",
    "                writer.writerow(data.keys())\n",
    "            writer.writerow(data.values())\n",
    "            csvfile.close()\n",
    "    except IOError:\n",
    "        print(\"I/O error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_average(numbers):\n",
    "    avg = []\n",
    "    for i in range(len(numbers)):\n",
    "        check = 0\n",
    "        l = i + 1\n",
    "        for j in range(i+1):\n",
    "            check += numbers[j]\n",
    "        avg.append(check/l)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB5LzSgfHr13"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VrZkNMGuoQy"
   },
   "outputs": [],
   "source": [
    "problems = {'DriverIdentification':{},\n",
    "            'ConfLongDemo_JSI':{},\n",
    "            'Healthy_Older_People':{},\n",
    "            'Motor_Failure_Time':{},\n",
    "            'Power_consumption':{},\n",
    "            'PRSA2017':{},\n",
    "            'RSSI':{},\n",
    "            'User_Identification_From_Walking':{},\n",
    "            'WISDM':{},\n",
    "           }\n",
    "\n",
    "problems['DriverIdentification']['dataset'] = './datasets/DriverIdentification/'\n",
    "problems['DriverIdentification']['n_classes'] = 10\n",
    "problems['DriverIdentification']['features'] = ['x-accelerometer', 'y-accelerometer', 'z-accelerometer', 'x-gyroscope', 'y-gyroscope', 'z-gyroscope']\n",
    "problems['DriverIdentification']['sample_rate'] = 2\n",
    "problems['DriverIdentification']['data_length_time'] = -1\n",
    "problems['DriverIdentification']['n_h_block'] = 15\n",
    "problems['DriverIdentification']['n_train_h_block'] = 9\n",
    "problems['DriverIdentification']['n_valid_h_block'] = 2\n",
    "problems['DriverIdentification']['n_test_h_block'] = 4\n",
    "problems['DriverIdentification']['h_moving_step'] = 1\n",
    "problems['DriverIdentification']['MLP/segments_time'] = 6\n",
    "problems['DriverIdentification']['CNN_L/segments_time'] = 10\n",
    "# Nadam(0.001, 0.9)\n",
    "# patience=5\n",
    "\n",
    "problems['ConfLongDemo_JSI']['dataset'] = './datasets/ConfLongDemo_JSI/'\n",
    "problems['ConfLongDemo_JSI']['n_classes'] = 5\n",
    "problems['ConfLongDemo_JSI']['features'] = [\"x\", \"y\", \"z\"]\n",
    "problems['ConfLongDemo_JSI']['sample_rate'] = 30\n",
    "problems['ConfLongDemo_JSI']['data_length_time'] = -1\n",
    "problems['ConfLongDemo_JSI']['n_h_block'] = 10\n",
    "problems['ConfLongDemo_JSI']['n_train_h_block'] = 5\n",
    "problems['ConfLongDemo_JSI']['n_valid_h_block'] = 2\n",
    "problems['ConfLongDemo_JSI']['n_test_h_block'] = 3\n",
    "problems['ConfLongDemo_JSI']['h_moving_step'] = 1\n",
    "problems['ConfLongDemo_JSI']['MLP/segments_time'] = 4\n",
    "problems['ConfLongDemo_JSI']['CNN_L/segments_time'] = 3\n",
    "# Nadam(0.001, 0.9)\n",
    "# patience=5\n",
    "\n",
    "problems['Healthy_Older_People']['dataset'] = './datasets/Healthy_Older_People/'\n",
    "problems['Healthy_Older_People']['n_classes'] = 12\n",
    "problems['Healthy_Older_People']['features'] = [\"X\", \"Y\", \"Z\"]\n",
    "problems['Healthy_Older_People']['sample_rate'] = 1\n",
    "problems['Healthy_Older_People']['data_length_time'] = -1\n",
    "problems['Healthy_Older_People']['n_h_block'] = 10\n",
    "problems['Healthy_Older_People']['n_train_h_block'] = 5\n",
    "problems['Healthy_Older_People']['n_valid_h_block'] = 2\n",
    "problems['Healthy_Older_People']['n_test_h_block'] = 3\n",
    "problems['Healthy_Older_People']['h_moving_step'] = 1\n",
    "problems['Healthy_Older_People']['MLP/segments_time'] = 7\n",
    "problems['Healthy_Older_People']['CNN_L/segments_time'] = 10\n",
    "# Nadam(0.001, 0.9)\n",
    "# patience=8\n",
    "\n",
    "problems['Motor_Failure_Time']['dataset'] = './datasets/Motor_Failure_Time/'\n",
    "problems['Motor_Failure_Time']['n_classes'] = 3\n",
    "problems['Motor_Failure_Time']['features'] = ['x', 'y', 'z']\n",
    "problems['Motor_Failure_Time']['sample_rate'] = 18\n",
    "problems['Motor_Failure_Time']['data_length_time'] = -1\n",
    "problems['Motor_Failure_Time']['n_h_block'] = 10\n",
    "problems['Motor_Failure_Time']['n_train_h_block'] = 5\n",
    "problems['Motor_Failure_Time']['n_valid_h_block'] = 2\n",
    "problems['Motor_Failure_Time']['n_test_h_block'] = 3\n",
    "problems['Motor_Failure_Time']['h_moving_step'] = 1\n",
    "problems['Motor_Failure_Time']['MLP/segments_time'] = 3\n",
    "problems['Motor_Failure_Time']['CNN_L/segments_time'] = 4\n",
    "# Nadam(0.001, 0.5)\n",
    "# patience=5\n",
    "\n",
    "problems['Power_consumption']['dataset'] = './datasets/Power_consumption/'\n",
    "problems['Power_consumption']['n_classes'] = 3\n",
    "problems['Power_consumption']['features'] = ['Temperature', 'Humidity', 'Wind Speed',\n",
    "                                             'general diffuse flows', 'diffuse flows',\n",
    "                                             'Consumption']\n",
    "problems['Power_consumption']['sample_rate'] = 1\n",
    "problems['Power_consumption']['data_length_time'] = -1\n",
    "problems['Power_consumption']['n_h_block'] = 15\n",
    "problems['Power_consumption']['n_train_h_block'] = 9\n",
    "problems['Power_consumption']['n_valid_h_block'] = 2\n",
    "problems['Power_consumption']['n_test_h_block'] = 4\n",
    "problems['Power_consumption']['h_moving_step'] = 1\n",
    "problems['Power_consumption']['MLP/segments_time'] = 180\n",
    "problems['Power_consumption']['CNN_L/segments_time'] = 420\n",
    "# Nadam(0.001, 0.9)\n",
    "# patience=10\n",
    "\n",
    "problems['PRSA2017']['dataset'] = './datasets/PRSA2017/'\n",
    "problems['PRSA2017']['n_classes'] = 12\n",
    "problems['PRSA2017']['features'] = ['PM2.5','PM10','SO2','NO2','CO','O3','TEMP','PRES','DEWP','RAIN','wd','WSPM']\n",
    "problems['PRSA2017']['sample_rate'] = 1\n",
    "problems['PRSA2017']['data_length_time'] = -1\n",
    "problems['PRSA2017']['n_h_block'] = 10\n",
    "problems['PRSA2017']['n_train_h_block'] = 5\n",
    "problems['PRSA2017']['n_valid_h_block'] = 2\n",
    "problems['PRSA2017']['n_test_h_block'] = 3\n",
    "problems['PRSA2017']['h_moving_step'] = 1\n",
    "problems['PRSA2017']['MLP/segments_time'] = 30\n",
    "problems['PRSA2017']['CNN_L/segments_time'] = 60\n",
    "# Nadam(0.001, 0.9)\n",
    "# patience=10\n",
    "\n",
    "problems['RSSI']['dataset'] = './datasets/RSSI/'\n",
    "problems['RSSI']['n_classes'] = 12\n",
    "problems['RSSI']['features'] = ['rssiOne', 'rssiTwo']\n",
    "problems['RSSI']['sample_rate'] = 1\n",
    "problems['RSSI']['data_length_time'] = -1\n",
    "problems['RSSI']['n_h_block'] = 10\n",
    "problems['RSSI']['n_train_h_block'] = 5\n",
    "problems['RSSI']['n_valid_h_block'] = 2\n",
    "problems['RSSI']['n_test_h_block'] = 3\n",
    "problems['RSSI']['h_moving_step'] = 1\n",
    "problems['RSSI']['MLP/segments_time'] = 60\n",
    "problems['RSSI']['CNN_L/segments_time'] = 60\n",
    "\n",
    "problems['User_Identification_From_Walking']['dataset'] = './datasets/User_Identification_From_Walking/'\n",
    "problems['User_Identification_From_Walking']['n_classes'] = 13\n",
    "problems['User_Identification_From_Walking']['features'] = [' x acceleration', ' y acceleration', ' z acceleration']\n",
    "problems['User_Identification_From_Walking']['sample_rate'] = 32\n",
    "problems['User_Identification_From_Walking']['data_length_time'] = -1\n",
    "problems['User_Identification_From_Walking']['n_h_block'] = 10\n",
    "problems['User_Identification_From_Walking']['n_train_h_block'] = 5\n",
    "problems['User_Identification_From_Walking']['n_valid_h_block'] = 2\n",
    "problems['User_Identification_From_Walking']['n_test_h_block'] = 3\n",
    "problems['User_Identification_From_Walking']['h_moving_step'] = 1\n",
    "problems['User_Identification_From_Walking']['MLP/segments_time'] = 3\n",
    "problems['User_Identification_From_Walking']['CNN_L/segments_time'] = 3\n",
    "\n",
    "problems['WISDM']['dataset'] = './datasets/WISDM/'\n",
    "problems['WISDM']['n_classes'] = 10\n",
    "problems['WISDM']['features'] = ['X-accel', 'Y-accel', 'Z-accel']\n",
    "problems['WISDM']['sample_rate'] = 20\n",
    "problems['WISDM']['data_length_time'] = -1\n",
    "problems['WISDM']['n_h_block'] = 10\n",
    "problems['WISDM']['n_train_h_block'] = 5\n",
    "problems['WISDM']['n_valid_h_block'] = 2\n",
    "problems['WISDM']['n_test_h_block'] = 3\n",
    "problems['WISDM']['h_moving_step'] = 1\n",
    "problems['WISDM']['MLP/segments_time'] = 4\n",
    "problems['WISDM']['CNN_L/segments_time'] = 8\n",
    "# Nadam(0.001, 0.9)\n",
    "# patience=5\n",
    "# epoch 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQwhNwXvqs-f"
   },
   "outputs": [],
   "source": [
    "train_config = [\n",
    "    {\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"loss_metric\": pbs,\n",
    "        \"callback\": \"ModelCheckpoint\",\n",
    "        \"monitor_metric\": \"val_pbs\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"loss_metric\": pbs,\n",
    "        \"callback\": \"EarlyStopping\",\n",
    "        \"monitor_metric\": \"val_pbs\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"loss_metric\": bs,\n",
    "        \"callback\": \"ModelCheckpoint\",\n",
    "        \"monitor_metric\": \"val_bs\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"mean_squared_error\",\n",
    "        \"loss_metric\": bs,\n",
    "        \"callback\": \"EarlyStopping\",\n",
    "        \"monitor_metric\": \"val_bs\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"categorical_crossentropy\",\n",
    "        \"loss_metric\": pll,\n",
    "        \"callback\": \"ModelCheckpoint\",\n",
    "        \"monitor_metric\": \"val_pll\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"categorical_crossentropy\",\n",
    "        \"loss_metric\": pll,\n",
    "        \"callback\": \"EarlyStopping\",\n",
    "        \"monitor_metric\": \"val_pll\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"categorical_crossentropy\",\n",
    "        \"loss_metric\": ll,\n",
    "        \"callback\": \"ModelCheckpoint\",\n",
    "        \"monitor_metric\": \"val_ll\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    },\n",
    "    {\n",
    "        \"loss_function\": \"categorical_crossentropy\",\n",
    "        \"loss_metric\": ll,\n",
    "        \"callback\": \"EarlyStopping\",\n",
    "        \"monitor_metric\": \"val_ll\",\n",
    "        \"monitor_mode\": \"min\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rSg-Y0euoQy"
   },
   "source": [
    "# Classic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lyNTFQNg8wQm",
    "outputId": "7a706721-8521-41a5-83aa-7a665165839b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise_rate = 100 # 100 means without noise\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "models = ['CNN_L']\n",
    "repetitions = 1\n",
    "\n",
    "decision_time = 0 # It is for recurrent models\n",
    "decision_overlap = 0\n",
    "\n",
    "# 'DriverIdentification','ConfLongDemo_JSI','Healthy_Older_People','Motor_Failure_Time','Power_consumption','PRSA2017','RSSI','User_Identification_From_Walking','WISDM'\n",
    "datasets = ['Motor_Failure_Time']\n",
    "for model in models:\n",
    "    for problem in datasets:\n",
    "        log_dir = f\"./log/{problem}/classic/\"\n",
    "        dataset = problems[problem]['dataset']\n",
    "        n_classes = problems[problem]['n_classes']\n",
    "        features = problems[problem]['features']\n",
    "        sample_rate = problems[problem]['sample_rate']\n",
    "        data_length_time = problems[problem]['data_length_time']\n",
    "        n_h_block = problems[problem]['n_h_block']\n",
    "        n_train_h_block = problems[problem]['n_train_h_block']\n",
    "        n_valid_h_block = problems[problem]['n_valid_h_block']\n",
    "        n_test_h_block = problems[problem]['n_test_h_block']\n",
    "        h_moving_step = problems[problem]['h_moving_step']\n",
    "        # train config index\n",
    "        for tci in [0,2,1,3]:\n",
    "            segments_time = problems[problem][model + '/segments_time']\n",
    "            segments_overlap = 0.75\n",
    "            classifier = eval(model)(classes=n_classes,\n",
    "                                     n_features=len(features),\n",
    "                                     segments_size=int(segments_time * sample_rate),\n",
    "                                     segments_overlap=segments_overlap,\n",
    "                                     decision_size=int(decision_time * sample_rate),\n",
    "                                     decision_overlap=decision_overlap,\n",
    "                                     loss_metric=train_config[tci][\"loss_metric\"],\n",
    "                                     loss_function=train_config[tci][\"loss_function\"])\n",
    "            # cross-validation\n",
    "            start = datetime.now()\n",
    "            statistics = h_block_analyzer(db_path=dataset,\n",
    "                                          sample_rate=sample_rate,\n",
    "                                          features=features,\n",
    "                                          n_classes=n_classes,\n",
    "                                          noise_rate=noise_rate,\n",
    "                                          segments_time=segments_time,\n",
    "                                          segments_overlap=segments_overlap,\n",
    "                                          decision_time=decision_time,\n",
    "                                          decision_overlap=decision_overlap,\n",
    "                                          classifier=classifier,\n",
    "                                          epochs=epochs,\n",
    "                                          batch_size=batch_size,\n",
    "                                          data_length_time=data_length_time,\n",
    "                                          repetitions=repetitions,\n",
    "                                          n_h_block=n_h_block,\n",
    "                                          n_train_h_block=n_train_h_block,\n",
    "                                          n_valid_h_block=n_valid_h_block,\n",
    "                                          n_test_h_block=n_test_h_block,\n",
    "                                          h_moving_step=h_moving_step,\n",
    "                                          callback=train_config[tci][\"callback\"],\n",
    "                                          monitor_metric=train_config[tci][\"monitor_metric\"],\n",
    "                                          monitor_mode=train_config[tci][\"monitor_mode\"])\n",
    "            end = datetime.now()\n",
    "            running_time = end - start\n",
    "            # Summarizing the results of cross-validation\n",
    "            data = {}\n",
    "            data['dataset'] = dataset\n",
    "            data['class'] = str(n_classes)\n",
    "            data['callback'] = train_config[tci][\"callback\"]\n",
    "            loss_metric = train_config[tci][\"loss_metric\"]\n",
    "            data['loss_metric'] = loss_metric if type(loss_metric) == str else loss_metric.__name__\n",
    "            loss_function = train_config[tci][\"loss_function\"]\n",
    "            data['loss_function'] = loss_function if type(loss_function) == str else loss_function.__name__\n",
    "            data['monitor_metric'] = train_config[tci][\"monitor_metric\"]\n",
    "            data['monitor_mode'] = train_config[tci][\"monitor_mode\"]\n",
    "            data['features'] = str(features)\n",
    "            data['sample_rate'] = str(sample_rate)\n",
    "            data['noise_rate'] = str(noise_rate)\n",
    "            data['epochs'] = str(epochs)\n",
    "            data['batch_size'] = str(batch_size)\n",
    "            data['data_length_time'] = str(data_length_time)\n",
    "            data['repetitions'] = str(repetitions)\n",
    "            data['n_h_block'] = str(n_h_block)\n",
    "            data['n_train_h_block'] = str(n_train_h_block)\n",
    "            data['n_valid_h_block'] = str(n_valid_h_block)\n",
    "            data['n_test_h_block'] = str(n_test_h_block)\n",
    "            data['h_moving_step'] = str(h_moving_step)\n",
    "            data['segments_time'] = str(segments_time)\n",
    "            data['segments_overlap'] = str(segments_overlap)\n",
    "            data['inner_classifier'] = str(model)\n",
    "            data['datetime'] = datetime.now().strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "            data['running_time'] = str(running_time.seconds) + \" seconds\"\n",
    "            data['n_params'] = classifier.count_params()\n",
    "            data['segments_time'] = timedelta(seconds=int(segments_time))\n",
    "            data['segments_overlap'] = segments_overlap\n",
    "            data['decision_time'] = timedelta(seconds=int(decision_time))\n",
    "            data['decision_overlap'] = decision_overlap\n",
    "            statistics_summary = {}\n",
    "            for key in statistics.keys():\n",
    "                statistics_summary[key + '_mean'] = np.nanmean(statistics[key])\n",
    "                statistics_summary[key + '_std'] = np.nanstd(statistics[key])\n",
    "                statistics_summary[key + '_max'] = np.nanmax(statistics[key])\n",
    "                statistics_summary[key + '_min'] = np.nanmin(statistics[key])\n",
    "            data.update(statistics_summary)\n",
    "            # Save information\n",
    "            save_result(log_dir=log_dir, data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFUG-JKcuoQ0"
   },
   "outputs": [],
   "source": [
    "print(\"End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZIm0OmusVgXS",
    "-Ng6rf-7xygn",
    "VbUrAsY1enTC",
    "Kn2SeTarrYuU",
    "1ddmcuoxK-Ij",
    "K5eGF7uXuoQu",
    "_w5A0xO9Upk1",
    "2bB8dITyHh5H",
    "caa9SgQ8HmsI",
    "lS6yHM-MHosi",
    "yqRhRm3V3SjM",
    "GB5LzSgfHr13",
    "ATTSjUsdqt1A",
    "9rSg-Y0euoQy",
    "tgN_ZoBKuoQz"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf_m1",
   "language": "python",
   "name": "tf_m1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
